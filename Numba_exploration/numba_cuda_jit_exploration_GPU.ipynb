{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed4684f-7b38-4e67-81e1-b79b40299ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b1faa0-d819-4418-bc26-6f7c23a8d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max threads per block - 1024\n"
     ]
    }
   ],
   "source": [
    "##Some GPU metrics\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "\n",
    "print(f\"Max threads per block - {device.MAX_THREADS_PER_BLOCK}\") ##Work with this as this \n",
    "                                            #is required for locating idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609401a8-13d9-479e-ae4e-4e4a9bc38939",
   "metadata": {},
   "source": [
    "#### Basic 1st GPU kernel using cuda jit from numbba - Square tthe elements of array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb95ebe8-136a-4cb6-90f5-ae2aa263a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@cuda.jit\n",
    "def square_kernel(ip, op):\n",
    "    idx = cuda.grid(1) #ip is 1D\n",
    "\n",
    "    if idx < ip.size: ## bounds check\n",
    "        op[idx] = ip[idx] ** 2   ## Note that this is not a looping thing\n",
    "\n",
    "\n",
    "##Data on CPU\n",
    "ip = np.arange(1000000, dtype=np.int64)\n",
    "op = np.zeros_like(ip)\n",
    "\n",
    "##Copy data to GPU\n",
    "d_arr = cuda.to_device(ip)\n",
    "d_result = cuda.to_device(op)\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks_req = (ip.size + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "square_kernel[blocks_req, threads_per_block](d_arr, d_result)\n",
    "\n",
    "result = d_result.copy_to_host()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0cf6cd-9735-42ed-9ed8-f78f1eace4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del  d_result, ip, op, result, d_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63015865-4946-41a6-b335-1c0ed7dcd6a0",
   "metadata": {},
   "source": [
    "#### Element-wise ops -start with addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c22dcea-a79b-4c71-a5d7-47a4700d4ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.59339559 0.79031274 1.47804362 0.53435669 1.34245197 0.05528387\n",
      " 1.59969826 1.76894895 0.6627682  1.60367803]\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def add_arrays(a, b, arr_sum):\n",
    "    idx = cuda.grid(1)\n",
    "\n",
    "    if idx < arr_sum.size: ## this should probably do bounds check for all i/o\n",
    "        arr_sum[idx] = a[idx] + b[idx]\n",
    "\n",
    "\n",
    "##Create arrays on CPU\n",
    "ip1 = np.random.rand(1000000)\n",
    "ip2 = np.random.rand(1000000)\n",
    "\n",
    "#Given this is size check its easier to assert this on CPU itself\n",
    "assert ip1.size == ip2.size\n",
    "\n",
    "res = np.zeros_like(ip1)\n",
    "arr_sum = cuda.to_device(res)\n",
    "\n",
    "\n",
    "threads_per_block = 1024\n",
    "blocks = (ip1.size + threads_per_block - 1) // threads_per_block\n",
    "add_arrays[blocks, threads_per_block](cuda.to_device(ip1), \n",
    "                                      cuda.to_device(ip2),\n",
    "                                      arr_sum)\n",
    "\n",
    "\n",
    "print((arr_sum.copy_to_host()[:10]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b67fbb7-d34f-47a8-a7f7-17f02857d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ip1, ip2, res, arr_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f2e2f9-bb34-414c-bf25-073a34566dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4dafa-3e85-4cd1-940e-2507d6bc0cfd",
   "metadata": {},
   "source": [
    "##### 2 D matrix addition - This is a lot easier than reduction operations in cuda jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3fd3509-ca03-4b54-a339-5464982ac4d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "CudaAPIError",
     "evalue": "[2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:851\u001b[0m, in \u001b[0;36mHostOnlyCUDAMemoryManager._attempt_allocation\u001b[0;34m(self, allocator)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mallocator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CudaAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# is out-of-memory?\u001b[39;00m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:1062\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc.<locals>.allocator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mallocator\u001b[39m():\n\u001b[0;32m-> 1062\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuMemAlloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:327\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    326\u001b[0m retcode \u001b[38;5;241m=\u001b[39m libfn(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_ctypes_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretcode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:395\u001b[0m, in \u001b[0;36mDriver._check_ctypes_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_fork()\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m CudaAPIError(retcode, msg)\n",
      "\u001b[0;31mCudaAPIError\u001b[0m: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m blocks_y \u001b[38;5;241m=\u001b[39m (A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m threads_per_block[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m threads_per_block[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     21\u001b[0m res \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mto_device(C)\n\u001b[1;32m     23\u001b[0m add_2d_matrices[(blocks_x, blocks_y), threads_per_block](cuda\u001b[38;5;241m.\u001b[39mto_device(A),\n\u001b[0;32m---> 24\u001b[0m                                                         \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     25\u001b[0m                                                         res)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(res\u001b[38;5;241m.\u001b[39mcopy_to_host()[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py:232\u001b[0m, in \u001b[0;36mrequire_context.<locals>._require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_require_cuda_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws):\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _runtime\u001b[38;5;241m.\u001b[39mensure_context():\n\u001b[0;32m--> 232\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkws\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/api.py:128\u001b[0m, in \u001b[0;36mto_device\u001b[0;34m(obj, stream, copy, to)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"to_device(obj, stream=0, copy=True, to=None)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03mAllocate and transfer a numpy ndarray or structured scalar to the device.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    hary = d_ary.copy_to_host(stream=stream)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     to, new \u001b[38;5;241m=\u001b[39m \u001b[43mdevicearray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43muser_explicit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m to\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/devicearray.py:878\u001b[0m, in \u001b[0;36mauto_device\u001b[0;34m(obj, stream, copy, user_explicit)\u001b[0m\n\u001b[1;32m    873\u001b[0m     obj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    874\u001b[0m         obj,\n\u001b[1;32m    875\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m numpy_version \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    876\u001b[0m         subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    877\u001b[0m     sentry_contiguous(obj)\n\u001b[0;32m--> 878\u001b[0m     devobj \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_array_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mCUDA_WARN_ON_IMPLICIT_COPY:\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/devicearray.py:799\u001b[0m, in \u001b[0;36mfrom_array_like\u001b[0;34m(ary, stream, gpu_data)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_array_like\u001b[39m(ary, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, gpu_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate a DeviceNDArray object that is like ary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDeviceNDArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mgpu_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/devicearray.py:104\u001b[0m, in \u001b[0;36mDeviceNDArrayBase.__init__\u001b[0;34m(self, shape, strides, dtype, stream, gpu_data)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malloc_size \u001b[38;5;241m=\u001b[39m _driver\u001b[38;5;241m.\u001b[39mmemory_size_from_info(\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 104\u001b[0m     gpu_data \u001b[38;5;241m=\u001b[39m \u001b[43mdevices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemalloc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malloc_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malloc_size \u001b[38;5;241m=\u001b[39m _driver\u001b[38;5;241m.\u001b[39mdevice_memory_size(gpu_data)\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:1372\u001b[0m, in \u001b[0;36mContext.memalloc\u001b[0;34m(self, bytesize)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmemalloc\u001b[39m(\u001b[38;5;28mself\u001b[39m, bytesize):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemalloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytesize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:1064\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mallocator\u001b[39m():\n\u001b[1;32m   1062\u001b[0m         driver\u001b[38;5;241m.\u001b[39mcuMemAlloc(byref(ptr), size)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attempt_allocation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallocator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m     alloc_key \u001b[38;5;241m=\u001b[39m ptr\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m   1067\u001b[0m finalizer \u001b[38;5;241m=\u001b[39m _alloc_finalizer(\u001b[38;5;28mself\u001b[39m, ptr, alloc_key, size)\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:863\u001b[0m, in \u001b[0;36mHostOnlyCUDAMemoryManager._attempt_allocation\u001b[0;34m(self, allocator)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeallocations\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;66;03m# try again\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mallocator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:1062\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc.<locals>.allocator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mallocator\u001b[39m():\n\u001b[0;32m-> 1062\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuMemAlloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:327\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    325\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall driver api: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, libfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    326\u001b[0m retcode \u001b[38;5;241m=\u001b[39m libfn(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_ctypes_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretcode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GPU_programming_for_DS/.venv/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py:395\u001b[0m, in \u001b[0;36mDriver._check_ctypes_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retcode \u001b[38;5;241m==\u001b[39m enums\u001b[38;5;241m.\u001b[39mCUDA_ERROR_NOT_INITIALIZED:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_fork()\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m CudaAPIError(retcode, msg)\n",
      "\u001b[0;31mCudaAPIError\u001b[0m: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY"
     ]
    }
   ],
   "source": [
    "@cuda.jit(fastmath=True)  ##allow fastmath\n",
    "def add_2d_matrices(A, B, C):\n",
    "\n",
    "    i,j = cuda.grid(2)\n",
    "\n",
    "    if i < A.shape[0] and j < A.shape[1]:\n",
    "        C[i,j] = A[i,j] + B[i,j]\n",
    "\n",
    "\n",
    "A = np.random.rand(10000, 10000)\n",
    "B = np.random.rand(10000, 10000)\n",
    "C = np.zeros_like(A)\n",
    "\n",
    "\n",
    "assert A.shape == B.shape \n",
    "\n",
    "threads_per_block = (32, 32)  ##32*32=1024\n",
    "blocks_x = (A.shape[0] + threads_per_block[0] - 1) // threads_per_block[0] # Blocks along 2 axes - #This is a visual illusion. In reality this doesnot exist \n",
    "blocks_y = (A.shape[1] + threads_per_block[1] - 1) // threads_per_block[1]\n",
    "\n",
    "res = cuda.to_device(C)\n",
    "\n",
    "add_2d_matrices[(blocks_x, blocks_y), threads_per_block](cuda.to_device(A),\n",
    "                                                        cuda.to_device(B),\n",
    "                                                        res)\n",
    "\n",
    "print(res.copy_to_host()[0][:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de3c1c-3048-4719-b09d-9e89c7ed82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del A, B, C, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f98f6d-bd86-471a-8a43-0a8d86b1588a",
   "metadata": {},
   "source": [
    "##### Reduction type operations - This felt a bit difficult to understand\n",
    "##### Honestly cupy is a lot more easier for this, if builtin ones are not helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b73b85-4235-4eba-967f-6d25d90b2920",
   "metadata": {},
   "source": [
    "##### Numba atomic operations - \n",
    "Usually used to prevent race conditions when you have want to read or write updates correctly. Mostly used when doing reduction type operations. Key tip is try to leverage shared memory to do this as opposed to global memory on GPU to facilitate faster read/write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5154c-5fc7-4846-8aa0-4de19960638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def sum_array(a, op):\n",
    "\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < a.size:\n",
    "        cuda.atomic.add(op, 0, a[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5f70f-d0c8-4222-a6d5-d65dca20530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random(1000000).astype(np.float64)\n",
    "d_result = cuda.to_device(np.array([0.0], dtype=np.float64))\n",
    "\n",
    "tpb = 1024\n",
    "blocks = (a.size + tpb - 1) // tpb\n",
    "sum_array[blocks, tpb](cuda.to_device(a),d_result)\n",
    "print(d_result.copy_to_host())\n",
    "\n",
    "##Note play around with floats -> using float32 in the kernel and np.sum is internally\n",
    "##float64 and given that these are floats due to roudning you will get a different answer\n",
    "\n",
    "##In this example use float32 in kernel you will get a different answer with a larg diffewrnce with np.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273ea67-0c3d-42a9-9e72-d7709b0fe28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c019089-4171-406a-8fca-142bee60d0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9e7ac-350c-412e-8ffa-159789444105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
