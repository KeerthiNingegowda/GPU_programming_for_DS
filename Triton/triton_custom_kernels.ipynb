{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cbc753-b657-4dcb-85e2-4f090a470b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b628cbfb-4635-49ca-82f8-ae8a6e2d23e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17770b1-12ce-45d0-83f0-fb32675c07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add 2 vectors - 1D - c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b76344a-5cb1-4e9e-87e7-f94779c22ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_two_vectors(ip1_ptr, ip2_ptr, op_ptr, n_elements, BLOCK_SIZE:tl.constexpr):\n",
    "\n",
    "    pid = tl.program_id(0)\n",
    "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    masks = offset < n_elements\n",
    "    ip1 = tl.load(ip1_ptr+offset, mask=masks)\n",
    "    ip2 = tl.load(ip2_ptr+offset, mask=masks)\n",
    "\n",
    "    op = ip1+ip2\n",
    "\n",
    "    tl.store(op_ptr+offset, op, mask=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb15095-dd64-47f6-ae20-396d34ff1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_add_kernel(x,y):\n",
    "    assert x.size() == y.size()\n",
    "\n",
    "    out = torch.zeros_like(x)\n",
    "    n_elements = x.numel()\n",
    "\n",
    "    ##Launch kernels parallely - well for triton its at grid level from a conceptyual standpoint\n",
    "    grid = lambda meta : (triton.cdiv(n_elements, meta['BLOCK_SIZE']),) #dont forget ,\n",
    "    add_two_vectors[grid](x, y, out, n_elements, BLOCK_SIZE=1024)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb579a7-ca35-4a25-9567-4f98a3f5bdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4691, -1.2322,  1.2438,  ..., -0.4972,  0.1273,  0.0804],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(8)\n",
    "SIZE = int(1e4)\n",
    "\n",
    "x = torch.randn(SIZE, device='cuda', dtype=torch.float32)\n",
    "y = torch.randn(SIZE, device='cuda', dtype=torch.float32)\n",
    "\n",
    "print(launch_add_kernel(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b38807-f737-447a-bb91-752da558d49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c161129d-bdcd-483e-b468-927b44734a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Square and add - out = x ^ 2 + y\n",
    "\n",
    "@triton.jit\n",
    "def square_and_add(ip1_ptr, ip2_ptr, out_ptr, n_elements, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0) # 1D array\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    masks = offsets < n_elements\n",
    "\n",
    "    x = tl.load(ip1_ptr + offsets, mask=masks)\n",
    "    y = tl.load(ip2_ptr + offsets, mask=masks)\n",
    "\n",
    "    out = x * x + y\n",
    "\n",
    "    tl.store(out_ptr + offsets, out, mask=masks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4717de0d-8e61-4cc2-be78-dde4868b7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_square_and_add():\n",
    "    x = torch.tensor([1,2,3,4], device=\"cuda\", dtype=torch.float32)\n",
    "    y = torch.tensor([10,20,30,40], device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    out = torch.zeros_like(x)\n",
    "    n_elements = x.numel()\n",
    "\n",
    "    grid = lambda meta : (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    square_and_add[grid](x, y, out, n_elements, BLOCK_SIZE=1024)\n",
    "\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c846e453-3acd-4dca-ae0e-194dddfe39cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11., 24., 39., 56.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "launch_square_and_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17826713-b800-489c-b19e-06f1bf05df08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//\n",
      "// Generated by LLVM NVPTX Back-End\n",
      "//\n",
      "\n",
      ".version 8.7\n",
      ".target sm_89\n",
      ".address_size 64\n",
      "\n",
      "\t// .globl\tsquare_and_add          // -- Begin function square_and_add\n",
      "                                        // @square_and_add\n",
      ".visible .entry square_and_add(\n",
      "\t.param .u64 .ptr .global .align 1 square_and_add_param_0,\n",
      "\t.param .u64 .ptr .global .align 1 square_and_add_param_1,\n",
      "\t.param .u64 .ptr .global .align 1 square_and_add_param_2,\n",
      "\t.param .u32 square_and_add_param_3,\n",
      "\t.param .u64 .ptr .global .align 1 square_and_add_param_4\n",
      ")\n",
      ".reqntid 128\n",
      "{\n",
      "\t.reg .pred \t%p<25>;\n",
      "\t.reg .b32 \t%r<39>;\n",
      "\t.reg .b64 \t%rd<29>;\n",
      "\t.loc\t1 4 0                           // 593028606.py:4:0\n",
      "$L__func_begin0:\n",
      "\t.loc\t1 4 0                           // 593028606.py:4:0\n",
      "\n",
      "// %bb.0:\n",
      "\tld.param.b64 \t%rd25, [square_and_add_param_0];\n",
      "\tld.param.b64 \t%rd26, [square_and_add_param_1];\n",
      "$L__tmp0:\n",
      "\t.loc\t1 5 24                          // 593028606.py:5:24\n",
      "\tmov.u32 \t%r25, %ctaid.x;\n",
      "\t.loc\t1 6 20                          // 593028606.py:6:20\n",
      "\tshl.b32 \t%r26, %r25, 10;\n",
      "\tld.param.b64 \t%rd27, [square_and_add_param_2];\n",
      "\tld.param.b32 \t%r27, [square_and_add_param_3];\n",
      "\t.loc\t1 6 46                          // 593028606.py:6:46\n",
      "\tmov.u32 \t%r28, %tid.x;\n",
      "\tshl.b32 \t%r29, %r28, 2;\n",
      "\tand.b32 \t%r30, %r29, 508;\n",
      "\t.loc\t1 6 33                          // 593028606.py:6:33\n",
      "\tor.b32 \t%r31, %r30, %r26;\n",
      "\tor.b32 \t%r32, %r31, 1;\n",
      "\tor.b32 \t%r33, %r31, 2;\n",
      "\tor.b32 \t%r34, %r31, 3;\n",
      "\tor.b32 \t%r35, %r31, 512;\n",
      "\tor.b32 \t%r36, %r31, 513;\n",
      "\tor.b32 \t%r37, %r31, 514;\n",
      "\tor.b32 \t%r38, %r31, 515;\n",
      "\t.loc\t1 7 22                          // 593028606.py:7:22\n",
      "\tsetp.lt.s32 \t%p1, %r31, %r27;\n",
      "\tsetp.lt.s32 \t%p2, %r32, %r27;\n",
      "\tsetp.lt.s32 \t%p3, %r33, %r27;\n",
      "\tsetp.lt.s32 \t%p4, %r34, %r27;\n",
      "\tsetp.lt.s32 \t%p5, %r35, %r27;\n",
      "\tsetp.lt.s32 \t%p6, %r36, %r27;\n",
      "\tsetp.lt.s32 \t%p7, %r37, %r27;\n",
      "\tsetp.lt.s32 \t%p8, %r38, %r27;\n",
      "\t.loc\t1 9 26                          // 593028606.py:9:26\n",
      "\tmul.wide.s32 \t%rd28, %r31, 4;\n",
      "\tadd.s64 \t%rd1, %rd25, %rd28;\n",
      "\tadd.s64 \t%rd2, %rd1, 4;\n",
      "\tadd.s64 \t%rd3, %rd1, 8;\n",
      "\tadd.s64 \t%rd4, %rd1, 12;\n",
      "\tadd.s64 \t%rd5, %rd1, 2048;\n",
      "\tadd.s64 \t%rd6, %rd1, 2052;\n",
      "\tadd.s64 \t%rd7, %rd1, 2056;\n",
      "\tadd.s64 \t%rd8, %rd1, 2060;\n",
      "\t.loc\t1 9 16                          // 593028606.py:9:16\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r1, 0x0;\n",
      "\t@%p1 ld.global.b32 { %r1 }, [ %rd1 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r2, 0x0;\n",
      "\t@%p2 ld.global.b32 { %r2 }, [ %rd2 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r3, 0x0;\n",
      "\t@%p3 ld.global.b32 { %r3 }, [ %rd3 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r4, 0x0;\n",
      "\t@%p4 ld.global.b32 { %r4 }, [ %rd4 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r5, 0x0;\n",
      "\t@%p5 ld.global.b32 { %r5 }, [ %rd5 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r6, 0x0;\n",
      "\t@%p6 ld.global.b32 { %r6 }, [ %rd6 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r7, 0x0;\n",
      "\t@%p7 ld.global.b32 { %r7 }, [ %rd7 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r8, 0x0;\n",
      "\t@%p8 ld.global.b32 { %r8 }, [ %rd8 + 0 ];\n",
      "\t// end inline asm\n",
      "\t.loc\t1 10 26                         // 593028606.py:10:26\n",
      "\tadd.s64 \t%rd9, %rd26, %rd28;\n",
      "\tadd.s64 \t%rd10, %rd9, 4;\n",
      "\tadd.s64 \t%rd11, %rd9, 8;\n",
      "\tadd.s64 \t%rd12, %rd9, 12;\n",
      "\tadd.s64 \t%rd13, %rd9, 2048;\n",
      "\tadd.s64 \t%rd14, %rd9, 2052;\n",
      "\tadd.s64 \t%rd15, %rd9, 2056;\n",
      "\tadd.s64 \t%rd16, %rd9, 2060;\n",
      "\t.loc\t1 10 16                         // 593028606.py:10:16\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r9, 0x0;\n",
      "\t@%p1 ld.global.b32 { %r9 }, [ %rd9 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r10, 0x0;\n",
      "\t@%p2 ld.global.b32 { %r10 }, [ %rd10 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r11, 0x0;\n",
      "\t@%p3 ld.global.b32 { %r11 }, [ %rd11 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r12, 0x0;\n",
      "\t@%p4 ld.global.b32 { %r12 }, [ %rd12 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r13, 0x0;\n",
      "\t@%p5 ld.global.b32 { %r13 }, [ %rd13 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r14, 0x0;\n",
      "\t@%p6 ld.global.b32 { %r14 }, [ %rd14 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r15, 0x0;\n",
      "\t@%p7 ld.global.b32 { %r15 }, [ %rd15 + 0 ];\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\tmov.u32 %r16, 0x0;\n",
      "\t@%p8 ld.global.b32 { %r16 }, [ %rd16 + 0 ];\n",
      "\t// end inline asm\n",
      "\t.loc\t1 12 18                         // 593028606.py:12:18\n",
      "\tfma.rn.f32 \t%r17, %r1, %r1, %r9;\n",
      "\tfma.rn.f32 \t%r18, %r2, %r2, %r10;\n",
      "\tfma.rn.f32 \t%r19, %r3, %r3, %r11;\n",
      "\tfma.rn.f32 \t%r20, %r4, %r4, %r12;\n",
      "\tfma.rn.f32 \t%r21, %r5, %r5, %r13;\n",
      "\tfma.rn.f32 \t%r22, %r6, %r6, %r14;\n",
      "\tfma.rn.f32 \t%r23, %r7, %r7, %r15;\n",
      "\tfma.rn.f32 \t%r24, %r8, %r8, %r16;\n",
      "\t.loc\t1 14 23                         // 593028606.py:14:23\n",
      "\tadd.s64 \t%rd17, %rd27, %rd28;\n",
      "\tadd.s64 \t%rd18, %rd17, 4;\n",
      "\tadd.s64 \t%rd19, %rd17, 8;\n",
      "\tadd.s64 \t%rd20, %rd17, 12;\n",
      "\tadd.s64 \t%rd21, %rd17, 2048;\n",
      "\tadd.s64 \t%rd22, %rd17, 2052;\n",
      "\tadd.s64 \t%rd23, %rd17, 2056;\n",
      "\tadd.s64 \t%rd24, %rd17, 2060;\n",
      "\t.loc\t1 14 32                         // 593028606.py:14:32\n",
      "\t// begin inline asm\n",
      "\t@%p1 st.global.b32 [ %rd17 + 0 ], { %r17 };\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\t@%p2 st.global.b32 [ %rd18 + 0 ], { %r18 };\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\t@%p3 st.global.b32 [ %rd19 + 0 ], { %r19 };\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\t@%p4 st.global.b32 [ %rd20 + 0 ], { %r20 };\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\t@%p5 st.global.b32 [ %rd21 + 0 ], { %r21 };\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\t@%p6 st.global.b32 [ %rd22 + 0 ], { %r22 };\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\t@%p7 st.global.b32 [ %rd23 + 0 ], { %r23 };\n",
      "\t// end inline asm\n",
      "\t// begin inline asm\n",
      "\t@%p8 st.global.b32 [ %rd24 + 0 ], { %r24 };\n",
      "\t// end inline asm\n",
      "\t.loc\t1 14 4                          // 593028606.py:14:4\n",
      "\tret;\n",
      "$L__tmp1:\n",
      "$L__func_end0:\n",
      "                                        // -- End function\n",
      "}\n",
      "\t.file\t1 \"/tmp/ipykernel_6206/593028606.py\"\n",
      "\t.section\t.debug_abbrev\n",
      "\t{\n",
      ".b8 1                                   // Abbreviation Code\n",
      ".b8 17                                  // DW_TAG_compile_unit\n",
      ".b8 0                                   // DW_CHILDREN_no\n",
      ".b8 37                                  // DW_AT_producer\n",
      ".b8 8                                   // DW_FORM_string\n",
      ".b8 19                                  // DW_AT_language\n",
      ".b8 5                                   // DW_FORM_data2\n",
      ".b8 3                                   // DW_AT_name\n",
      ".b8 8                                   // DW_FORM_string\n",
      ".b8 16                                  // DW_AT_stmt_list\n",
      ".b8 6                                   // DW_FORM_data4\n",
      ".b8 27                                  // DW_AT_comp_dir\n",
      ".b8 8                                   // DW_FORM_string\n",
      ".b8 0                                   // EOM(1)\n",
      ".b8 0                                   // EOM(2)\n",
      ".b8 0                                   // EOM(3)\n",
      "\t}\n",
      "\t.section\t.debug_info\n",
      "\t{\n",
      ".b32 54                                 // Length of Unit\n",
      ".b8 2                                   // DWARF version number\n",
      ".b8 0\n",
      ".b32 .debug_abbrev                      // Offset Into Abbrev. Section\n",
      ".b8 8                                   // Address Size (in bytes)\n",
      ".b8 1                                   // Abbrev [1] 0xb:0x2f DW_TAG_compile_unit\n",
      ".b8 116                                 // DW_AT_producer\n",
      ".b8 114\n",
      ".b8 105\n",
      ".b8 116\n",
      ".b8 111\n",
      ".b8 110\n",
      ".b8 0\n",
      ".b8 2                                   // DW_AT_language\n",
      ".b8 0\n",
      ".b8 53                                  // DW_AT_name\n",
      ".b8 57\n",
      ".b8 51\n",
      ".b8 48\n",
      ".b8 50\n",
      ".b8 56\n",
      ".b8 54\n",
      ".b8 48\n",
      ".b8 54\n",
      ".b8 46\n",
      ".b8 112\n",
      ".b8 121\n",
      ".b8 0\n",
      ".b32 .debug_line                        // DW_AT_stmt_list\n",
      ".b8 47                                  // DW_AT_comp_dir\n",
      ".b8 116\n",
      ".b8 109\n",
      ".b8 112\n",
      ".b8 47\n",
      ".b8 105\n",
      ".b8 112\n",
      ".b8 121\n",
      ".b8 107\n",
      ".b8 101\n",
      ".b8 114\n",
      ".b8 110\n",
      ".b8 101\n",
      ".b8 108\n",
      ".b8 95\n",
      ".b8 54\n",
      ".b8 50\n",
      ".b8 48\n",
      ".b8 54\n",
      ".b8 0\n",
      "\t}\n",
      "\t.section\t.debug_macinfo\t{\t}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lets check the PTX code. The caching method didnt work for me. \n",
    "\n",
    "from triton.compiler import CompiledKernel\n",
    "\n",
    "# Compile the kernel with specific parameters\n",
    "compiled = square_and_add.warmup(\n",
    "    torch.float32,  # ip1_ptr type\n",
    "    torch.float32,  # ip2_ptr type  \n",
    "    torch.float32,  # out_ptr type\n",
    "    4,              # n_elements\n",
    "    BLOCK_SIZE=1024,\n",
    "    grid=(1,)\n",
    ")\n",
    "\n",
    "# Get the PTX code\n",
    "ptx = compiled.asm['ptx']\n",
    "print(ptx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb7bc882-8795-4f97-bbb4-e44c8b4cc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Key insights - Really blown away. Like soo many optimizations done by Triton\n",
    "#automatically as opposed.\n",
    "\n",
    "#1) Usage of FMA ops - which is better than 2 instructions plus one add\n",
    "#2) Predictaed masking -  setp.lt.s32 ...  Unlike numba where your bound checks\n",
    "##are done using if conditions, triton avoids it completely by masking. Which is actually quite smart\n",
    "#One can get away with loop unrolling but still have to do ifs\n",
    "\n",
    "#3) Memory access - Contiguous between threads but strided across threads\n",
    "#Evidence - Adjacent threads getting adjacent offsets -> shift left by 2 bits operation shl.b32 %r29, %r28, 2  on %tid.x\n",
    "# stride access is 2048 bytes (512 elements) - add.s64 %rd5, %rd1, 2048\n",
    "# coalesced access - All threads execute same ld.global.b32 ..... - which is cool.\n",
    "\n",
    "##Vectorization - Each thread processes 8 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36dae4a-e6c8-42aa-be09-dd8a72013bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Element wise maximum with threshold - out = max(x, threshold)\n",
    "\n",
    "@triton.jit\n",
    "def max_with_threshold(x_ptr, threshold, out_ptr, n_elements, BLOCK_SIZE:tl.constexpr): \n",
    "    \n",
    "    #Scalars will be automatically broadcasted.But it is not like numpy or pytorch broadcasting\n",
    "    ## tensors will be strectched. But here scalars will be put in registers directtly\n",
    "\n",
    "    pid = tl.program_id(0)\n",
    "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    masks = offsets < n_elements\n",
    "\n",
    "    x = tl.load(x_ptr + offsets, mask=masks)\n",
    "\n",
    "    out = tl.maximum(x, threshold)\n",
    "\n",
    "    tl.store(out_ptr + offsets, out, mask=masks)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "342342d1-5da3-4f74-8e0e-c3468a13a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_max_with_threshold():\n",
    "    x = torch.tensor([-1, 0.3, 0.8, 1.5], device=\"cuda\", dtype=torch.float32)\n",
    "    threshold = 0.5\n",
    "    out = torch.zeros_like(x)\n",
    "    \n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta : (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]), )\n",
    "\n",
    "    max_with_threshold[grid](x, threshold, out, n_elements, BLOCK_SIZE=1024)\n",
    "\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88e24023-6046-4d7b-8ece-674128883e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 0.5000, 0.8000, 1.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "launch_max_with_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0948d4-482a-4891-aac3-8d5a5516b8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e38bc-2e6e-43c4-9b71-052444c83800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e525c-672b-48dc-92ca-2d661d565bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973aea70-5a09-47dc-94d0-a7c2b39f1708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb3541-ed62-40b8-b20e-afd3c40f3ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1800e03-9caa-4ec1-9906-7930e63b3099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30923eae-74b8-4dd1-abb8-bf65b2b8b73d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
